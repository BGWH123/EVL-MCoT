# EVL-MCoT

Welcome to the official repository for **EVL-MCoT** â€“ a project focused on enhanced visual-linguistic reasoning using Multi-step Chain-of-Thought (MCoT) prompting.

---

## ðŸ“‚ Structure Overview

This repository currently includes the core code needed to begin development and experimentation.

- **`train/`**: Contains the main training pipeline. This includes model initialization, training loop, logging, and evaluation scripts. You can customize training behavior by modifying parameters or integrating your dataset here.
- **`models/`** *(coming soon)*: Model architecture definitions, including encoders and decoding heads.
- **`configs/`** *(planned)*: Configuration files for reproducible experiments.
- **`scripts/`** *(upcoming)*: Tools for data preprocessing, visualization, and evaluation.

---

## ðŸš€ Project Goal

The **EVL-MCoT** project aims to enhance multi-modal reasoning by integrating visual information with structured, multi-step chain-of-thought prompts. This helps models reason more effectively and interpretably across vision-language tasks.

---

## ðŸ“Œ Current Status

The core training code is available in the `train/` directory. Other components including model definitions, sample data, and configuration files will be added in upcoming updates.

---

## ðŸ”œ Upcoming Updates

- [ ] Usage instructions and command-line interface
- [ ] Environment and dependency setup (e.g., `requirements.txt`)
- [ ] Example dataset and visualization demos
- [ ] Pretrained model weights
- [ ] Model training and evaluation instructions

---


## ðŸ“¬ Stay Tuned!

More documentation and detailed examples are coming soon. If you're interested in vision-language models, interpretable reasoning, or chain-of-thought prompting â€” this project is for you!

---
